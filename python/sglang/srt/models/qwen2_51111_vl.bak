# coding=utf-8
# Adapted from
# https://github.com/huggingface/transformers/blob/19e6e80e10118f855137b90740936c0b11ac397f/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py
# Copyright 2024 The Qwen team.
# Copyright 2023 The vLLM team.
# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Inference-only Qwen2-VL model compatible with HuggingFace weights."""
import logging
from functools import lru_cache, partial
from typing import Iterable, List, Optional, Tuple, Type

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from transformers.activations import ACT2FN
from transformers.models.qwen2.modeling_qwen2 import Qwen2RMSNorm
from transformers.models.qwen2_5_vl.configuration_qwen2_5_vl import (
    Qwen2_5_VLConfig,
    Qwen2_5_VLVisionConfig,
)
from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (
    Qwen2_5_VisionPatchEmbed,
    Qwen2_5_VisionRotaryEmbedding,
)

from sglang.srt.distributed import parallel_state
from sglang.srt.hf_transformers_utils import get_processor
from sglang.srt.layers.attention.vision import VisionAttention
from sglang.srt.layers.linear import ColumnParallelLinear, RowParallelLinear
from sglang.srt.layers.logits_processor import LogitsProcessor
from sglang.srt.layers.pooler import Pooler, PoolingType
from sglang.srt.layers.quantization.base_config import QuantizationConfig
from sglang.srt.layers.vocab_parallel_embedding import ParallelLMHead
from sglang.srt.managers.mm_utils import (
    MultiModalityDataPaddingPatternMultimodalTokens,
    general_mm_embed_routine,
)
from sglang.srt.managers.schedule_batch import MultimodalDataItem, MultimodalInputs
from sglang.srt.model_executor.forward_batch_info import ForwardBatch, PPProxyTensors
from sglang.srt.model_loader.weight_utils import default_weight_loader
from sglang.srt.models.qwen2 import Qwen2Model
from sglang.srt.models.qwen2_vl import Qwen2VLVideoInputs
from sglang.srt.utils import add_prefix

logger = logging.getLogger(__name__)


# Add back the PPMissingLayer for non-last PP ranks
class PPMissingLayer(nn.Module):
    """A placeholder layer for pipeline parallelism used in non-last ranks."""

    def __init__(self):
        super().__init__()

    def forward(self, *args, **kwargs):
        return None

    @property
    def weight(self):
        return None


class Qwen2_5_VLMLP(nn.Module):
    def __init__(
        self,
        in_features: int,
        hidden_features: int = None,
        bias: bool = True,
        hidden_act="silu",
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = "",
    ):
        super().__init__()
        self.gate_proj = ColumnParallelLinear(
            in_features,
            hidden_features,
            bias=bias,
            quant_config=quant_config,
            prefix=add_prefix("gate_proj", prefix),
        )
        self.up_proj = ColumnParallelLinear(
            in_features,
            hidden_features,
            bias=bias,
            quant_config=quant_config,
            prefix=add_prefix("up_proj", prefix),
        )
        self.down_proj = RowParallelLinear(
            hidden_features,
            in_features,
            bias=bias,
            quant_config=quant_config,
            prefix=add_prefix("down_proj", prefix),
        )
        self.act = ACT2FN[hidden_act]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_parallel_gate, _ = self.gate_proj(x)
        x_parallel_gate = self.act(x_parallel_gate)
        x_parallel_up, _ = self.up_proj(x)
        x_parallel = x_parallel_gate * x_parallel_up
        x, _ = self.down_proj(x_parallel)
        return x


class Qwen2_5_VisionBlock(nn.Module):
    def __init__(
        self,
        dim: int,
        intermediate_dim: int,
        num_heads: int,
        hidden_act="silu",
        norm_layer: Type[nn.Module] = None,
        attn_implementation: Optional[str] = "sdpa",
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = "",
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = partial(nn.LayerNorm, eps=1e-6)
        self.norm1 = Qwen2RMSNorm(dim, eps=1e-6)
        self.norm2 = Qwen2RMSNorm(dim, eps=1e-6)
        if attn_implementation == "sdpa":
            softmax_in_single_precision = False
            qkv_backend = "sdpa"
            flatten_batch = True
        elif attn_implementation == "flash_attention_2":
            softmax_in_single_precision = False
            qkv_backend = "triton_attn"
            flatten_batch = True
        elif attn_implementation == "eager":
            softmax_in_single_precision = True
            qkv_backend = "sdpa"
            flatten_batch = True
        elif attn_implementation == "flash_attention_3":
            softmax_in_single_precision = False
            qkv_backend = "fa3"
            flatten_batch = True

        self.attn = VisionAttention(
            embed_dim=dim,
            num_heads=num_heads,
            projection_size=dim,
            use_qkv_parallel=True,
            rotary_embed="normal",
            proj_bias=True,
            qkv_backend=qkv_backend,
            softmax_in_single_precision=softmax_in_single_precision,
            flatten_batch=flatten_batch,
            quant_config=quant_config,
            prefix=add_prefix("attn", prefix),
        )
        self.mlp = Qwen2_5_VLMLP(
            dim,
            intermediate_dim,
            hidden_act=hidden_act,
            quant_config=quant_config,
            prefix=add_prefix("mlp", prefix),
        )

    def forward(
        self,
        x: torch.Tensor,
        cu_seqlens: torch.Tensor,
        position_embeddings: torch.Tensor,
    ) -> torch.Tensor:
        hidden_states = self.norm1(x)
        hidden_states = rearrange(hidden_states, "s b ... -> b s ...")
        attn = self.attn(
            hidden_states,
            cu_seqlens=cu_seqlens,
            position_embeddings=position_embeddings,
        )
        attn = rearrange(attn, "b s ... -> s b ...")
        x = x + attn
        norm2 = self.norm2(x)
        mlp = self.mlp(norm2)
        x = x + mlp
        return x


class Qwen2_5_VisionPatchMerger(nn.Module):
    def __init__(
        self,
        dim: int,
        context_dim: int,
        spatial_merge_size: int = 2,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = "",
    ) -> None:
        super().__init__()
        self.hidden_size = context_dim * (spatial_merge_size**2)
        self.ln_q = Qwen2RMSNorm(context_dim, eps=1e-6)
        self.mlp = nn.ModuleList(
            [
                ColumnParallelLinear(
                    self.hidden_size,
                    self.hidden_size,
                    bias=True,
                    quant_config=quant_config,
                    prefix=add_prefix("mlp.0", prefix),
                ),
                nn.GELU(),
                RowParallelLinear(
                    self.hidden_size,
                    dim,
                    bias=True,
                    quant_config=quant_config,
                    prefix=add_prefix("mlp.2", prefix),
                ),
            ]
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.ln_q(x)
        x = x.view(-1, self.hidden_size)

        mlp_fc1, mlp_act, mlp_fc2 = self.mlp
        x_parallel, _ = mlp_fc1(x)
        x_parallel = mlp_act(x_parallel)
        out, _ = mlp_fc2(x_parallel)
        return out


class Qwen2_5_VisionTransformer(nn.Module):
    def __init__(
        self,
        vision_config: Qwen2_5_VLVisionConfig,
        norm_eps: float = 1e-6,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = "",
    ) -> None:
        super().__init__()
        self.config = vision_config
        embed_dim = vision_config.hidden_size

        self.patch_embed = Qwen2_5_VisionPatchEmbed(
            hidden_size=embed_dim,
            patch_size=vision_config.patch_size,
            in_channels=vision_config.in_channels,
            quant_config=quant_config,
            prefix=add_prefix("patch_embed", prefix),
        )
        self.pos_embed = nn.Parameter(
            torch.randn(
                1,
                (vision_config.image_size // vision_config.patch_size) ** 2,
                embed_dim,
            )
        )
        self.layers = nn.ModuleList(
            [
                Qwen2_5_VisionBlock(
                    dim=embed_dim,
                    intermediate_dim=vision_config.intermediate_size,
                    num_heads=vision_config.num_attention_heads,
                    norm_layer=partial(Qwen2RMSNorm, eps=norm_eps),
                    attn_implementation=vision_config._attn_implementation,
                    quant_config=quant_config,
                    prefix=add_prefix(f"layers.{i}", prefix),
                )
                for i in range(vision_config.num_hidden_layers)
            ]
        )

        self.merger = nn.ModuleList()
        # image dynamic resolution
        self.patch_size = vision_config.patch_size
        self.image_size = vision_config.image_size
        self.thw_shape = vision_config.thw_shape
        self.window_size = vision_config.window_size
        self.num_windows = (self.image_size // self.patch_size) // self.window_size

        self.rotary_emb = Qwen2_5_VisionRotaryEmbedding(
            vision_config.hidden_size // vision_config.num_attention_heads,
            base=vision_config.rope_base,
        )

    def get_window_index(self, grid_thw):
        # ... (the rest of the class is unchanged)
        S = self.num_windows
        # Generate the indices for the RoPE embeddings
        q_indices = torch.arange(S * S, dtype=torch.long, device=grid_thw.device)
        q_indices = q_indices.view(S, S)
        k_indices = q_indices

        # Get the row and column indices for q and k
        q_row, q_col = q_indices // S, q_indices % S
        k_row, k_col = k_indices // S, k_indices % S

        # Reshape to prepare for broadcasting
        q_row = q_row.view(S, S, 1, 1)
        q_col = q_col.view(S, S, 1, 1)
        k_row = k_row.view(1, 1, S, S)
        k_col = k_col.view(1, 1, S, S)

        # Calculate the displacement
        disp_row = q_row - k_row
        disp_col = q_col - k_col

        # Scale the displacement by the window size
        scaled_disp_row = disp_row * self.window_size
        scaled_disp_col = disp_col * self.window_size

        # Create the index tensor
        index = torch.stack([scaled_disp_row, scaled_disp_col], dim=-1)

        # Handle the grid_thw input
        B = grid_thw.size(0)
        h_indices = torch.clamp(
            grid_thw[:, 0, 0] // self.patch_size - self.window_size // 2,
            0,
            S - self.window_size,
        )
        w_indices = torch.clamp(
            grid_thw[:, 0, 1] // self.patch_size - self.window_size // 2,
            0,
            S - self.window_size,
        )

        # Gather the embeddings
        h_idx = h_indices.view(B, 1, 1, 1, 1).expand(-1, S, S, S, S, -1)
        w_idx = w_indices.view(B, 1, 1, 1, 1).expand(-1, S, S, S, S, -1)
        final_index = index.unsqueeze(0).expand(B, -1, -1, -1, -1, -1)
        final_index[..., 0] += h_idx
        final_index[..., 1] += w_idx
        return final_index

    @property
    def dtype(self) -> torch.dtype:
        return self.patch_embed.proj.weight.dtype

    @property
    def device(self) -> torch.device:
        return self.patch_embed.proj.weight.device

    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
        self.rotary_emb.to(grid_thw.device)
        B, L, H, W, D = (
            grid_thw.shape[0],
            self.window_size,
            self.window_size,
            self.window_size,
            self.window_size,
        )
        # get window index
        window_indices = self.get_window_index(grid_thw).view(-1, 2)
        pos_emb = self.rotary_emb(window_indices).view(
            B, L * H, W * D, -1
        )  # B, S*S, S*S, D
        return pos_emb

    def forward(
        self,
        x: torch.Tensor,
        grid_thw: torch.Tensor,
    ) -> torch.Tensor:
        # patchify
        x = self.patch_embed(x)
        x = x + self.pos_embed.to(x.device)
        B = x.shape[0]

        # reshape to window
        x = x.view(
            B,
            self.num_windows,
            self.window_size,
            self.num_windows,
            self.window_size,
            -1,
        )
        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()
        x = x.view(B, self.num_windows**2, self.window_size**2, -1)

        position_embeddings = self.rot_pos_emb(grid_thw)
        x = rearrange(x, "b s l d -> (b s) l d")
        position_embeddings = rearrange(position_embeddings, "b s k d -> (b s) k d")
        cu_seqlens = torch.arange(
            0,
            (x.shape[0] + 1) * x.shape[1],
            step=x.shape[1],
            dtype=torch.int32,
            device=x.device,
        )

        for layer in self.layers:
            x = layer(x, cu_seqlens, position_embeddings)

        x = rearrange(x, "(b s) l d -> b s l d", b=B)

        # un-window and merge
        x = x.view(
            B,
            self.num_windows,
            self.num_windows,
            self.window_size,
            self.window_size,
            -1,
        )
        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()
        x = x.view(B, self.num_windows * self.window_size, -1)
        x = self.merger(x)
        return x


class Qwen2_5_VLForConditionalGeneration(nn.Module):
    # BitandBytes specific attributes
    default_bitsandbytes_target_modules = [
        ".gate_proj.",
        ".down_proj.",
        ".up_proj.",
        ".q_proj.",
        ".k_proj.",
        ".v_proj.",
        ".o_proj.",
    ]
    bitsandbytes_stacked_params_mapping = {
        "qkv_proj": ("qkv_proj", "q_proj", "k_proj", "v_proj"),
        "gate_up_proj": ("gate_up_proj", "gate_proj", "up_proj"),
    }

    def __init__(
        self,
        config: Qwen2_5_VLConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = "",
    ) -> None:
        super().__init__()
        self.config = config
        self.pp_group = parallel_state.get_pipeline_model_parallel_group()
        self.pp_rank = parallel_state.get_pp_rank()
        self.pp_size = parallel_state.get_pp_world_size()

        self.model = Qwen2Model(
            config, quant_config=quant_config, prefix=add_prefix("model", prefix)
        )

        if self.pp_rank == self.pp_size - 1:
            self.lm_head = ParallelLMHead(
                config.vocab_size,
                config.hidden_size,
                quant_config=quant_config,
                prefix=add_prefix("lm_head", prefix),
            )
        else:
            self.lm_head = PPMissingLayer()

        self.logits_processor = LogitsProcessor(config)
        self.image_token_idx = (
            get_processor(config._name_or_path, use_fast=False)
            .get_image_processor()
            .image_token_index
        )
        if self.pp_rank == 0:
            self.visual = Qwen2_5_VisionTransformer(
                config.vision_config,
                norm_eps=getattr(config, "rms_norm_eps", 1e-6),
                quant_config=quant_config,
                prefix=add_prefix("visual", prefix),
            )
        else:
            # The other PP ranks use a placeholder layer to avoid occupying GPU memory / back-propagating params.
            self.visual = PPMissingLayer()

    def _bcast_tensor(self, t: torch.Tensor) -> torch.Tensor:
        """
        Broadcast a 2-D tensor (seq, hidden) within the PP group.
        Stage-0 sends, and the other stages first apply for an empty tensor of the same shape and then receive.
        """
        if self.pp_size == 1:
            return t

        seq_len = torch.tensor([t.size(0)], device=t.device, dtype=torch.int32)
        # broadcast length
        dist.broadcast(seq_len, src=0, group=self.pp_group)
        if self.pp_rank != 0:  # non-0-rank create buffer
            t = torch.empty(
                int(seq_len.item()),
                self.config.hidden_size,
                device="cuda",
                dtype=self.model.dtype,
            )
        # broadcast data
        dist.broadcast(t, src=0, group=self.pp_group)
        return t

    def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs):
        # Get all special token IDs
        image_token_id = self.image_token_idx

        new_input_ids = MultiModalityDataPaddingPatternMultimodalTokens(
            input_ids, image_token_id, mm_inputs.pixel_values.shape[0]
        ).get_input_ids_with_padding()
        return new_input_ids

    def get_image_feature(self, items: List[MultimodalDataItem]) -> torch.Tensor:
        """
        Return the (total_seq, hidden) Tensor concatenated in the order of items.
        - PP0 is responsible for the real calculation
        - The other PP ranks get the same result through NCCL broadcast
        """
        if self.pp_rank == 0:
            pixel_values = torch.cat([it.pixel_values for it in items], dim=0).to(
                self.visual.dtype
            )
            grid_thw = torch.cat([it.image_grid_thw for it in items], dim=0)
            embeds = self.visual(pixel_values, grid_thw)
            embeds_flat = embeds.contiguous()
        else:
            embeds_flat = torch.empty(
                1, self.config.hidden_size, device="cuda", dtype=self.model.dtype
            )

        embeds_flat = self._bcast_tensor(embeds_flat)

        return embeds_flat

    def _process_video_input(self, video_input: Qwen2VLVideoInputs) -> torch.Tensor:
        # Not implemented yet
        raise NotImplementedError("Video input is not supported yet.")

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        get_embedding: bool = False,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
    ):
        if pp_proxy_tensors is not None and "hidden_states" in pp_proxy_tensors.tensors:
            # The input is from the previous stage
            hidden_states = pp_proxy_tensors.tensors["hidden_states"]
        elif get_embedding:
            return self.model.embed_tokens(input_ids)
        elif forward_batch.mm_data is not None:
            hidden_states = general_mm_embed_routine(
                self, input_ids, self.model.embed_tokens, forward_batch
            )
        else:
            hidden_states = self.model.embed_tokens(input_ids)

        hidden_states = self.model(
            input_ids=None,
            positions=positions,
            input_embeds=hidden_states,
            forward_batch=forward_batch,
        )

        if self.pp_rank != self.pp_size - 1:
            return PPProxyTensors({"hidden_states": hidden_states})

        if get_embedding:
            return self.pooler(hidden_states, forward_batch)
        else:
            logits = self.logits_processor(
                input_ids, hidden_states, self.lm_head, forward_batch
            )
            return logits

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        # Define the prefix map: {checkpoint_prefix: sglang_model_prefix}
        prefix_map = {
            "model.": "model.",
            "lm_head.": "lm_head.",
            "visual.": "visual.",
        }
        # Find the corresponding sglang model prefix for each weight
        params_dict = dict(self.named_parameters())
        for name, loaded_weight in weights:
            if self.pp_rank != 0 and name.startswith("visual."):
                continue

            for old_prefix, new_prefix in prefix_map.items():
                if name.startswith(old_prefix):
                    name = new_prefix + name[len(old_prefix) :]
                    break
            
            # The logic for stacked parameters is not needed here
            # because the weight loader handles it.
            if name not in params_dict:
                logger.warning(
                    f"Weight loading failed. Skipped weight: '{name}'. "
                    f"The corresponding parameter was not found on this rank (PP rank {self.pp_rank})."
                )
                continue
            
            param = params_dict[name]
            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(param, loaded_weight)


EntryClass = [Qwen2_5_VLForConditionalGeneration]